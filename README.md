# anonymousCodeContext

![Task formulation](task_formulation.png)

`anonymousCodeContext` is a comprehensive benchmark framework for code generation and evaluation, supporting multiple context engineering strategies including similarity-based retrieval, static analysis, and navigation methods. The project supports LLMs and provides a complete pipeline including prompt generation, code generation, and evaluation.

## Features

-   **Context Retrieval Methods**: Supports multiple strategies for obtaining context, including navigation, static analysis, similarity-based retrieval, and baseline approaches.
-   **Multi-LLM Support**: Integrated with major LLMs including GPT, Gemini, and Claude.
-   **End-to-End Evaluation Pipeline**: A complete workflow from data preprocessing to code generation and automated testing.
-   **Flexible Configuration**: Utilizes a YAML-based configuration system with support for dynamic parameter substitution.
-   **Resumable Execution**: Supports resuming experiments from any step, enhancing efficiency.


## Core Architecture

### Main Components

- **`gen_and_eval.py`**: The main entry point that orchestrates the entire workflow.
- **`gen_process_elements.py`, `gen_similarity.py`, `gen_navigation2.py`, `gen_static.py`**: Modules for preprocessing, including element extraction, similarity retrieval, navigation-based, and static analysis-based context generation.
- **`make_prompt.py`**: Generates prompts for code generation tasks.
- **`langChain_inference_async.py`**: Handles concurrent LLM inference for code generation.
- **`func_extractor.py`**: Post-processes code generated by LLMs, including function extraction.
- **`eval_pass_k.py`**: Evaluation script for DevEval, calculates pass@k and runs test cases.
- **`navigation/`, `navigation_plus_python/`**: Provide LLM tools for navigation-based context engineering.
- **`na_utils/`**: Utilities for checkpointing and parallel LLM invocation.
- **`data/`**: Data directory containing datasets and intermediate files.
- **`config/`**: YAML configuration files used by `gen_and_eval.py`.

### Execution Flow

```
Data Preprocessing → Prompt Generation → Code Generation → Post-processing → Evaluation
```

## Getting Started

### Prerequisites

-   Python 3.9+

### Installation

You can set up the environment using the provided `conda_environment.yml` file:

```bash
conda env create -f conda_environment.yml
conda activate Navigation
```

## Usage

### 1. Configuration

Create a configuration file based on `config/your_config_name.yaml`:

```yaml
Exec:
  preprocess:    1  # 0: Skip, 1: Continue, 2: Rerun
  make_prompt:   1
  generate_code: 1
  postprocess:   1
  eval:          1

MODEL: gemini  # gpt, gemini, claude
TASK: similarity_bm25
MODA: sampling

Dataset:
    ...
Preprocess:
    ...
MakePrompt:
    ...
GenerateCode:
    ...
Postprocess:
    ...
Eval:
    ...
Stat:
    ...
```

We provide detailed explanations in config/baseline_gpt.yaml.

### 2. Run Experiment

```bash
# for DevEval
python gen_and_eval.py --config config/baseline_gpt.yaml
# for CoderEval
python gen_and_eval.py --config config/coderEval/baseline_gpt.yaml
```

## Advanced Features

### Dynamic Configuration Substitution

The framework supports complex and nested parameter substitution in YAML configuration files, allowing for highly dynamic and reusable configurations.

```yaml
# Basic substitution
output_path: ${EXEC_DATA_TASK_PATH}/preprocess/elements.jsonl

# Nested substitution
preprocess_file: ${Preprocess.${Preprocess.type}.output_path}
```

### Resumable Execution

Both code generation and testing steps support resumable execution. The framework checks for the existence of output files to determine whether a step should be skipped or re-executed, based on the configuration.

## Static Analysis for Context Generation

This project includes a static analysis-based context generation capability to fairly evaluate different Context Engineering strategies. The goal is to create context from intra-project dependencies related to functions/methods in a target file, excluding the target function itself and any standard library or external dependencies.

### Components

-   `ast_analyzer/visitor.py`: An AST visitor that operates at the function/method scope to collect raw "dependency events" (e.g., `call`, `method_call`, `attribute_access`). It includes robust local variable detection to minimize misinterpretation.
-   `ast_analyzer/analyzer.py`: Resolves dependency events into semantic dependencies (`intra_class`, `cross_file`, `property`, etc.). It supports class inheritance, `@property` decorators, and traces `self.<attr>` back to its `__init__` assignment.
-   `codebase_indexer.py`: A tree-sitter based code indexer for precise extraction of function, class, or module-level variable source code snippets. It includes caching to improve performance for repeated analyses on the same codebase.
-   `gen_static.py`: An end-to-end script that orchestrates the context generation process: analyzes dependencies for all functions in a file (except the target), filters out external libraries, and extracts relevant source code to build the final context.

### Usage

-   **Run the built-in example:**
    ```bash
    python gen_static.py | tee output.txt
    ```
-   **Call as a function:**
    ```python
    from gen_static import generate_context_for_file

    context = generate_context_for_file(
        project_root="/abs/path/to/your/codebase",
        target_file_path="/abs/path/to/your/codebase/pkg/module.py",
        target_function_id="pkg/module.py::ClassName::method"  # or "pkg/module.py::function"
    )

    with open("analysis_result.txt", "w", encoding="utf-8") as f:
        f.write(context)
    ```



### Known Limitations

-   Static analysis cannot fully cover complex dynamic dispatches, reflection, or factory patterns that rely on runtime information.
-   Type inference for chained calls uses heuristics (e.g., guessing class from the RHS of an `__init__` assignment), which is effective but not exhaustive.



## Adding New LLM Tools to `navigation`

When adding new Tools for LLMs, especially in scenarios where multiple questions are processed concurrently (using coroutines or threads), it is essential to ensure data isolation between different tasks. This is particularly important for per-repository data such as `project_path`.

### Tool Implementation Pattern
Data isolation is achieved through the proxy pattern or the factory pattern, with reference to the `ToolImpl` class in `navigation/tools2.py`.


#### Example: `read_file` Tool
```python
@tool(parse_docstring=True)
def read_file(task_id: str, file_path: str, start_line: int=None, end_line: int=None) -> str:
    """
		docstring for LLM 
		Args:
        task_id: your task_id, which is a required parameter for all the provided tools.
        file_path: the path of the file to read
        ...
    """
    return ToolImpl.get_tools_instance(task_id).read_file(file_path, start_line, end_line)
```

The read_file function above only provides an interface for the LLM. The actual implementation is delegated to the `ToolImpl` class, which holds the isolated data (such as `project_path`) for each task.

```python
# ToolImpl.read_file
def read_file(self, file_path: str, start_line: int=None, end_line: int=None) -> str:
    proj_path = self.project_path
    a_file_path = os.path.join(proj_path, file_path)
    ...
    with open(a_file_path, 'r') as file:
        lines = file.readlines()
        # deal with lines
        return '\n'.join(lines)
```
### Stateful Tool Invocation

When an LLM performs a code generation task for a specific question, it typically makes multiple calls to tools we provided. We want to preserve the cross-tool state for this generation task, similar to how HTTP cookies or sessions work. We use `task_id` to index a specific generation task, and `AgentHelper.STATES[task_id]` is a hasd table that stores the state of that task. The `task_id` is provided to the LLM by our framework. If you add a new tool and want it to use the STATE, simply refer to the task_id parameter description in read_file tool above.




## Troubleshooting

### Common Issues

1.  **API Key Problems**:
    -   Ensure the API key file path is correct.
    -   Verify that the API key is valid.
2.  **Out of Memory**:
    -   Reduce the `max_concurrent` parameter.
3.  **Test Environment Issues**:
    -   Ensure all project dependencies are correctly installed.
    -   Check the virtual environment setup.

    demo:
    ```sh
    # for each repo
    cd path_to_repo
    python -m venv myenv && source myenv/bin/activate 
    pip install -e . 
    pip install pytest
    ```
